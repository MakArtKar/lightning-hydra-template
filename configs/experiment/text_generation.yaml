# @package _global_

# to execute this experiment run:
# python train.py experiment=text_generation

defaults:
  - /metrics@callbacks.metrics_callback: accuracy
  - /model/criterions: ce
  - /task/text: base
  - /model/forward_fn: ar
  - override /data: hf_dataset
  - override /model: base
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags:
  - text_generation
  - ${params.data.dataset.path}

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 2

data:
  batch_size: 64

model:
  criterions:
    _target_: ml_core.models.utils.CriterionsComposition

    mapping:
      ce:
        input: transposed_logits
        target: input_ids

callbacks:
  metrics_callback:
    mapping:
      accuracy:
        target: input_ids

  best_metric_tracker_callback:
    tracked_metric_name: # tracking val/total loss

params:
  data:
    dataset:
      path: wikitext
      name: wikitext-2-v1

    tokenizer:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: openai-community/gpt2
      pad_token: "<|endoftext|>"

    vocab_size:
      _target_: builtins.getattr
      _args_:
        - ${params.data.tokenizer}
        - vocab_size

    num_classes: ${params.data.vocab_size}

    max_len: 256
  
  model:
    network:
      hidden_size: 512
      num_layers: 6
      num_heads: 8

logger:
  wandb:
    tags: ${tags}
    group: text_generation
    name: text_generation-${params.data.dataset.path}
  aim:
    experiment: text_generation
